{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat','sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "mnist_train =torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=True, download=False, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='../Datasets/FashionMNIST', train=False, download=False, transform=transforms.ToTensor())\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: trouser')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 80\n",
    "plt.imshow(mnist_train[index][0].view(28, 28).numpy())\n",
    "plt.title(\"Label: \" + text_labels[mnist_train[index][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "    def backward(self, input, grad):\n",
    "        pass\n",
    "\n",
    "# 定义卷积层\n",
    "class Convolutional(Layer):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, k_size, stride=1, learning_rate=0.1):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.k_size = k_size\n",
    "        self.stride = stride\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = torch.normal(0, 0.1, (out_ch, in_ch, k_size, k_size), dtype=torch.float)\n",
    "        self.b = torch.zeros(out_ch, dtype=torch.float)\n",
    "        self.param = {}\n",
    "        \n",
    "    def im2col(self, input, n, c, s, p):\n",
    "        # 通过卷积的方式将输入的四维张量转化为二维张量\n",
    "        im = torch.zeros((n*p*p, self.k_size*self.k_size*c))\n",
    "        for i in range(p):\n",
    "            i_start = i*self.stride\n",
    "            i_end = i_start + self.k_size\n",
    "            for j in range(p):\n",
    "                j_start = j*self.stride\n",
    "                j_end = j_start + self.k_size\n",
    "                im[i*p+j::p*p, :] = input[:,:,i_start:i_end,j_start:j_end].reshape(n, -1)\n",
    "        return im\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # 前向传播\n",
    "        n, c, s, _ = input.shape\n",
    "        self.param['inputshape'] = input.shape\n",
    "        p = (s - self.k_size) // self.stride + 1\n",
    "        w = self.w.reshape(self.out_ch, self.k_size**2*c)\n",
    "        im = self.im2col(input, n, c, s, p)\n",
    "        output = torch.matmul(im, w.T) + self.b # 通过矩阵乘法来计算卷积\n",
    "        return output.reshape(n, self.out_ch, p, p)\n",
    "    \n",
    "    def paddings(self, input, pad):\n",
    "        # padding实现接口\n",
    "        n, c, s, _ = input.shape\n",
    "        outshape = pad*2 + s\n",
    "        out = torch.zeros((n, c, outshape, outshape))\n",
    "        out[:,:,pad:outshape-pad,pad:outshape-pad] = input\n",
    "        return out\n",
    "    \n",
    "    def nexteta(self, grad):\n",
    "        # 计算卷积层输入的误差项\n",
    "        gs = grad.shape[2]\n",
    "        ln, lc, ls, _ = self.param['inputshape']\n",
    "        pad = math.ceil((self.stride*(ls-1) + self.k_size - gs) / 2) # 反向传播时对卷积层输出数据增加padding\n",
    "        grad = self.paddings(grad, pad)\n",
    "        w = torch.flip(self.w, dims=[2,3]) # 卷积核翻转\n",
    "        \n",
    "        n, c, s, _ = grad.shape\n",
    "        p = (s - self.k_size) // self.stride + 1\n",
    "        w = w.reshape(lc, self.k_size**2*c)\n",
    "        im = self.im2col(grad, n, c, s, p)\n",
    "        eta = torch.matmul(im, w.T).T.reshape(n, lc, p, p)\n",
    "        return eta   \n",
    "        \n",
    "    def backward(self, input, grad):\n",
    "        # 反向传播，input为卷积层输入数据，grad为卷积层输出的误差项\n",
    "        dw = torch.zeros(self.w.shape)\n",
    "        db = torch.zeros(self.b.shape)\n",
    "        n, c, s, _ = grad.shape\n",
    "        for i in range(self.k_size):\n",
    "            for j in range(self.k_size):\n",
    "                cons = input[:,:,i*self.stride:i*self.stride+s,i*self.stride:i*self.stride+s]\n",
    "                for k in range(c):\n",
    "                    dw[k,:,i,j] = torch.sum(cons * (grad[:, k, :, :])[:, None], axis=(0,2,3)) / n\n",
    "        db = torch.sum(grad, axis=(0,2,3)) / n\n",
    "        self.w -= self.learning_rate*dw # 更新权重\n",
    "        self.b -= self.learning_rate*db # 更新偏置\n",
    "        eta = self.nexteta(grad)\n",
    "        return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPooling(Layer):\n",
    "    \n",
    "    def __init__(self, in_ch, k_size, stride):\n",
    "        self.in_ch = in_ch\n",
    "        self.stride = stride\n",
    "        self.k_size = k_size\n",
    "        self.param = {}\n",
    "    \n",
    "    def pool(self, input, n, c, s, p):\n",
    "        # 平均池化\n",
    "        pl = torch.zeros((n, c, p, p))\n",
    "        for i in range(n):\n",
    "            for j in range(c):\n",
    "                for k in range(p):\n",
    "                    for z in range(p):\n",
    "                        inp = input[i,j,k*self.stride:k*self.stride+self.k_size,z*self.stride:z*self.stride+self.k_size]\n",
    "                        pl[i,j,k,z] = torch.sum(inp) / (self.k_size*self.k_size)\n",
    "        return pl\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # 前向传播\n",
    "        self.param['inputshape'] = input.shape\n",
    "        n, c, s, _ = input.shape\n",
    "        p = (s - self.k_size) // self.stride + 1\n",
    "        output = self.pool(input, n, c, s, p)\n",
    "        self.param['outputshape'] = output.shape\n",
    "        return output\n",
    "       \n",
    "    def backward(self, input, grad):\n",
    "        # 反向传播\n",
    "        grad = grad.reshape(self.param['outputshape'])\n",
    "        n, c, p, _ = grad.shape\n",
    "        eta = torch.zeros(self.param['inputshape'])\n",
    "        for i in range(n):\n",
    "            for j in range(c):\n",
    "                for k in range(p):\n",
    "                    for z in range(p):\n",
    "                        eta[i,j,k*self.stride:k*self.stride+self.k_size,z*self.stride:z*self.stride+self.k_size]=grad[i,j,k,z]\n",
    "        return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(Layer):\n",
    "    \n",
    "    def __init__(self, in_ch, k_size, stride):\n",
    "        self.in_ch = in_ch\n",
    "        self.stride = stride\n",
    "        self.k_size = k_size\n",
    "        self.param = {}\n",
    "    \n",
    "    def pool(self, input, n, c, p):\n",
    "        # 最大池化\n",
    "        pl = torch.zeros((n, c, p, p))\n",
    "        index = []\n",
    "        for i in range(n):\n",
    "            for j in range(c):\n",
    "                for k in range(p):\n",
    "                    for z in range(p):\n",
    "                        inp = input[i,j,k*self.stride:k*self.stride+self.k_size,z*self.stride:z*self.stride+self.k_size]\n",
    "                        pl[i,j,k,z] = torch.max(inp)\n",
    "                        ix = torch.where(inp==pl[i,j,k,z])\n",
    "                        index.append((i,j,k*self.stride+ix[0],z*self.stride+ix[1])) # 记录最大值位置\n",
    "        self.param['index'] = index\n",
    "        return pl\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # 前向传播\n",
    "        self.param['shape'] = input.shape\n",
    "        n, c, s, _ = input.shape\n",
    "        p = (s - self.k_size) // self.stride + 1\n",
    "        output = self.pool(input, n, c, p)\n",
    "        return output\n",
    "       \n",
    "    def backward(self, input, grad):\n",
    "        # 反向传播\n",
    "        grad = grad.reshape(-1)\n",
    "        eta = torch.zeros(self.param['shape'])\n",
    "        for i in range(len(self.param['index'])):\n",
    "            eta[self.param['index'][i]] = grad[i]\n",
    "        return eta\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU激活函数\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input):\n",
    "        return (torch.abs(input) + input) / 2.0\n",
    "    def backward(self, input, grad):\n",
    "        input[input<=0] = 0\n",
    "        input[input>0] = 1\n",
    "        return grad*input\n",
    "\n",
    "# softmax激活函数\n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,input):\n",
    "        exp = torch.exp(input)\n",
    "        exp_sum = torch.sum(exp, axis=1, keepdims=True)\n",
    "        return exp / exp_sum\n",
    "    \n",
    "    def backward(self,input,grad):\n",
    "        return grad\n",
    "\n",
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, num_input, num_output, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = torch.normal(0, 0.1, (num_input, num_output), dtype=torch.float)\n",
    "        self.b = torch.zeros(num_output, dtype=torch.float)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.reshape(input.shape[0], -1)\n",
    "        return torch.mm(input, self.w) + self.b\n",
    "    \n",
    "    def backward(self, input, grad):\n",
    "        input = input.reshape(input.shape[0], -1)\n",
    "        grad_input = torch.mm(grad, self.w.T)\n",
    "        dw = torch.mm(input.T, grad) / input.shape[0]\n",
    "        db = torch.sum(grad, axis=0) / input.shape[0]\n",
    "        self.w -= self.learning_rate*dw\n",
    "        self.b -= self.learning_rate*db\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(logits, y):\n",
    "    # 交叉熵损失\n",
    "    loss = -1.0/y.shape[0] * torch.sum(y*torch.log(logits) + (1-y)*torch.log(1-logits))\n",
    "    loss_grad = logits-y\n",
    "    return loss, loss_grad\n",
    "\n",
    "def forward(network, x):\n",
    "    # 前向传播\n",
    "    activations = []\n",
    "    input = x\n",
    "    for layer in network:\n",
    "        #print(input)\n",
    "        activations.append(layer.forward(input))\n",
    "        input = activations[-1]\n",
    "                \n",
    "    return activations\n",
    "\n",
    "def predict(network, test_iter):\n",
    "    # 测试模型\n",
    "    n = 0\n",
    "    m = 0\n",
    "    for data in test_iter:\n",
    "        x, y = data\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        logits = forward(network,x)[-1]\n",
    "        n += torch.sum(torch.argmax(logits, axis=1) == y)\n",
    "        m += x.shape[0]\n",
    "    return n * 1.0 / m\n",
    "\n",
    "def predict(network, test_iter):\n",
    "    # 测试模型\n",
    "    n = 0\n",
    "    m = 0\n",
    "    for data in test_iter:\n",
    "        x, y = data\n",
    "        logits = forward(network,x)[-1]\n",
    "        n += torch.sum(torch.argmax(logits, axis=1) == y)\n",
    "        m += x.shape[0]\n",
    "    return n * 1.0 / m\n",
    "\n",
    "def train(network,x,y): \n",
    "    # 训练模型\n",
    "    layer_activations = forward(network,x)\n",
    "    layer_inputs = [x]+layer_activations  \n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    loss, loss_grad = cross_entropy(logits, y)\n",
    "    \n",
    "    for layer_i in range(len(network)-1)[::-1]:\n",
    "        layer = network[layer_i]\n",
    "        loss_grad = layer.backward(layer_inputs[layer_i],loss_grad) \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "network = []\n",
    "network.append(Convolutional(1,16,5,2,learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(MaxPooling(16,2,2))\n",
    "network.append(Dense(576,20,learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(20,10,learning_rate=learning_rate))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数为1，当前损失为3.247763156890869\n",
      "迭代次数为2，当前损失为3.2269370555877686\n",
      "迭代次数为3，当前损失为3.2213127613067627\n",
      "迭代次数为4，当前损失为3.219313621520996\n",
      "迭代次数为5，当前损失为3.1712541580200195\n",
      "迭代次数为6，当前损失为3.169743537902832\n",
      "迭代次数为7，当前损失为3.1413002014160156\n",
      "迭代次数为8，当前损失为3.1182732582092285\n",
      "迭代次数为9，当前损失为3.0814454555511475\n",
      "迭代次数为10，当前损失为3.0654525756835938\n",
      "迭代次数为11，当前损失为3.038463830947876\n",
      "迭代次数为12，当前损失为3.044112205505371\n",
      "迭代次数为13，当前损失为3.0051636695861816\n",
      "迭代次数为14，当前损失为3.0016350746154785\n",
      "迭代次数为15，当前损失为2.9422495365142822\n",
      "迭代次数为16，当前损失为2.880502939224243\n",
      "迭代次数为17，当前损失为2.8525595664978027\n",
      "迭代次数为18，当前损失为2.8430967330932617\n",
      "迭代次数为19，当前损失为2.8267016410827637\n",
      "迭代次数为20，当前损失为2.7731447219848633\n",
      "迭代次数为21，当前损失为2.737354278564453\n",
      "迭代次数为22，当前损失为2.7086944580078125\n",
      "迭代次数为23，当前损失为2.7168612480163574\n",
      "迭代次数为24，当前损失为2.641364097595215\n",
      "迭代次数为25，当前损失为2.610157012939453\n",
      "迭代次数为26，当前损失为2.587371587753296\n",
      "迭代次数为27，当前损失为2.5377426147460938\n",
      "迭代次数为28，当前损失为2.47664737701416\n",
      "迭代次数为29，当前损失为2.4868357181549072\n",
      "迭代次数为30，当前损失为2.3952319622039795\n",
      "迭代次数为31，当前损失为2.3334503173828125\n",
      "迭代次数为32，当前损失为2.3857789039611816\n",
      "迭代次数为33，当前损失为2.336310863494873\n",
      "迭代次数为34，当前损失为2.3186755180358887\n",
      "迭代次数为35，当前损失为2.2729341983795166\n",
      "迭代次数为36，当前损失为2.22137451171875\n",
      "迭代次数为37，当前损失为2.116542339324951\n",
      "迭代次数为38，当前损失为2.1559829711914062\n",
      "迭代次数为39，当前损失为2.1628079414367676\n",
      "迭代次数为40，当前损失为2.1273741722106934\n",
      "迭代次数为41，当前损失为2.0584490299224854\n",
      "迭代次数为42，当前损失为2.0183048248291016\n",
      "迭代次数为43，当前损失为1.9883748292922974\n",
      "迭代次数为44，当前损失为1.9665632247924805\n",
      "迭代次数为45，当前损失为1.9593595266342163\n",
      "迭代次数为46，当前损失为2.0337719917297363\n",
      "迭代次数为47，当前损失为1.9491252899169922\n",
      "迭代次数为48，当前损失为1.8299750089645386\n",
      "迭代次数为49，当前损失为1.9350025653839111\n",
      "迭代次数为50，当前损失为1.9106003046035767\n",
      "迭代次数为51，当前损失为1.9594459533691406\n",
      "迭代次数为52，当前损失为1.8221540451049805\n",
      "迭代次数为53，当前损失为1.825876235961914\n",
      "迭代次数为54，当前损失为1.7965221405029297\n",
      "迭代次数为55，当前损失为1.9577986001968384\n",
      "迭代次数为56，当前损失为1.876660704612732\n",
      "迭代次数为57，当前损失为1.8500714302062988\n",
      "迭代次数为58，当前损失为1.8133054971694946\n",
      "迭代次数为59，当前损失为1.7482924461364746\n",
      "迭代次数为60，当前损失为1.7430775165557861\n",
      "迭代次数为61，当前损失为1.6715730428695679\n",
      "迭代次数为62，当前损失为1.7345781326293945\n",
      "迭代次数为63，当前损失为1.7356122732162476\n",
      "迭代次数为64，当前损失为1.881557822227478\n",
      "迭代次数为65，当前损失为1.7587668895721436\n",
      "迭代次数为66，当前损失为1.7005021572113037\n",
      "迭代次数为67，当前损失为1.7606016397476196\n",
      "迭代次数为68，当前损失为1.6601911783218384\n",
      "迭代次数为69，当前损失为1.6299409866333008\n",
      "迭代次数为70，当前损失为1.5477439165115356\n",
      "迭代次数为71，当前损失为1.6635810136795044\n",
      "迭代次数为72，当前损失为1.6368094682693481\n",
      "迭代次数为73，当前损失为1.6025710105895996\n",
      "迭代次数为74，当前损失为1.7502135038375854\n",
      "迭代次数为75，当前损失为1.8327810764312744\n",
      "迭代次数为76，当前损失为2.064614772796631\n",
      "迭代次数为77，当前损失为1.656059741973877\n",
      "迭代次数为78，当前损失为1.6356604099273682\n",
      "迭代次数为79，当前损失为1.6711989641189575\n",
      "迭代次数为80，当前损失为1.638622522354126\n",
      "迭代次数为81，当前损失为1.5568827390670776\n",
      "迭代次数为82，当前损失为1.57221257686615\n",
      "迭代次数为83，当前损失为1.482852578163147\n",
      "迭代次数为84，当前损失为1.4535273313522339\n",
      "迭代次数为85，当前损失为1.4930365085601807\n",
      "迭代次数为86，当前损失为1.5753421783447266\n",
      "迭代次数为87，当前损失为1.6533395051956177\n",
      "迭代次数为88，当前损失为1.4980368614196777\n",
      "迭代次数为89，当前损失为1.403848648071289\n",
      "迭代次数为90，当前损失为1.5512511730194092\n",
      "迭代次数为91，当前损失为1.4673815965652466\n",
      "迭代次数为92，当前损失为1.4965355396270752\n",
      "迭代次数为93，当前损失为1.5742754936218262\n",
      "迭代次数为94，当前损失为1.986512303352356\n",
      "迭代次数为95，当前损失为1.9315683841705322\n",
      "迭代次数为96，当前损失为1.721718430519104\n",
      "迭代次数为97，当前损失为1.452521800994873\n",
      "迭代次数为98，当前损失为1.3595099449157715\n",
      "迭代次数为99，当前损失为1.4150583744049072\n",
      "迭代次数为100，当前损失为1.5863053798675537\n",
      "迭代次数为101，当前损失为1.5410206317901611\n",
      "迭代次数为102，当前损失为1.4805679321289062\n",
      "迭代次数为103，当前损失为1.5563411712646484\n",
      "迭代次数为104，当前损失为1.3941444158554077\n",
      "迭代次数为105，当前损失为1.439303994178772\n",
      "迭代次数为106，当前损失为1.5299803018569946\n",
      "迭代次数为107，当前损失为1.5769799947738647\n",
      "迭代次数为108，当前损失为1.5435984134674072\n",
      "迭代次数为109，当前损失为1.4100309610366821\n",
      "迭代次数为110，当前损失为1.4825880527496338\n",
      "迭代次数为111，当前损失为1.4329636096954346\n",
      "迭代次数为112，当前损失为1.4967143535614014\n",
      "迭代次数为113，当前损失为1.696746587753296\n",
      "迭代次数为114，当前损失为1.7935116291046143\n",
      "迭代次数为115，当前损失为1.5737358331680298\n",
      "迭代次数为116，当前损失为1.4989522695541382\n",
      "迭代次数为117，当前损失为1.4541786909103394\n",
      "迭代次数为118，当前损失为1.5593966245651245\n",
      "迭代次数为119，当前损失为1.569459319114685\n",
      "迭代次数为120，当前损失为1.4584845304489136\n",
      "迭代次数为121，当前损失为1.4432728290557861\n",
      "迭代次数为122，当前损失为1.2257459163665771\n",
      "迭代次数为123，当前损失为1.326149344444275\n",
      "迭代次数为124，当前损失为1.4373867511749268\n",
      "迭代次数为125，当前损失为1.5328539609909058\n",
      "迭代次数为126，当前损失为1.3784619569778442\n",
      "迭代次数为127，当前损失为1.4954864978790283\n",
      "迭代次数为128，当前损失为1.3224835395812988\n",
      "迭代次数为129，当前损失为1.4104913473129272\n",
      "迭代次数为130，当前损失为1.485339641571045\n",
      "迭代次数为131，当前损失为1.5657398700714111\n",
      "迭代次数为132，当前损失为1.489660382270813\n",
      "迭代次数为133，当前损失为1.8219120502471924\n",
      "迭代次数为134，当前损失为1.3311078548431396\n",
      "迭代次数为135，当前损失为1.429574728012085\n",
      "迭代次数为136，当前损失为1.1402126550674438\n",
      "迭代次数为137，当前损失为1.4766592979431152\n",
      "迭代次数为138，当前损失为1.476159691810608\n",
      "迭代次数为139，当前损失为1.2735978364944458\n",
      "迭代次数为140，当前损失为1.4884804487228394\n",
      "迭代次数为141，当前损失为1.5619184970855713\n",
      "迭代次数为142，当前损失为1.6359715461730957\n",
      "迭代次数为143，当前损失为1.5598475933074951\n",
      "迭代次数为144，当前损失为1.4998481273651123\n",
      "迭代次数为145，当前损失为1.3707337379455566\n",
      "迭代次数为146，当前损失为1.3070210218429565\n",
      "迭代次数为147，当前损失为1.457021951675415\n",
      "迭代次数为148，当前损失为1.3385353088378906\n",
      "迭代次数为149，当前损失为1.4961928129196167\n",
      "迭代次数为150，当前损失为1.3436230421066284\n",
      "迭代次数为151，当前损失为1.360114574432373\n",
      "迭代次数为152，当前损失为1.2833622694015503\n",
      "迭代次数为153，当前损失为1.6538752317428589\n",
      "迭代次数为154，当前损失为1.5572350025177002\n",
      "迭代次数为155，当前损失为1.3827996253967285\n",
      "迭代次数为156，当前损失为1.2929438352584839\n",
      "迭代次数为157，当前损失为1.3273838758468628\n",
      "迭代次数为158，当前损失为1.2487839460372925\n",
      "迭代次数为159，当前损失为1.259647011756897\n",
      "迭代次数为160，当前损失为1.2953181266784668\n",
      "迭代次数为161，当前损失为1.3922717571258545\n",
      "迭代次数为162，当前损失为1.2870477437973022\n",
      "迭代次数为163，当前损失为1.3624032735824585\n",
      "迭代次数为164，当前损失为1.3659411668777466\n",
      "迭代次数为165，当前损失为1.3688327074050903\n",
      "迭代次数为166，当前损失为1.4047194719314575\n",
      "迭代次数为167，当前损失为1.4380784034729004\n",
      "迭代次数为168，当前损失为1.3018999099731445\n",
      "迭代次数为169，当前损失为1.27297043800354\n",
      "迭代次数为170，当前损失为1.3097516298294067\n",
      "迭代次数为171，当前损失为1.335108757019043\n",
      "迭代次数为172，当前损失为1.1977277994155884\n",
      "迭代次数为173，当前损失为1.2831982374191284\n",
      "迭代次数为174，当前损失为1.6059951782226562\n",
      "迭代次数为175，当前损失为1.7796961069107056\n",
      "迭代次数为176，当前损失为1.3327627182006836\n",
      "迭代次数为177，当前损失为1.474367380142212\n",
      "迭代次数为178，当前损失为1.440385103225708\n",
      "迭代次数为179，当前损失为1.3229789733886719\n",
      "迭代次数为180，当前损失为1.5644285678863525\n",
      "迭代次数为181，当前损失为1.4095858335494995\n",
      "迭代次数为182，当前损失为1.405617117881775\n",
      "迭代次数为183，当前损失为1.2678685188293457\n",
      "迭代次数为184，当前损失为1.2600561380386353\n",
      "迭代次数为185，当前损失为1.3196336030960083\n",
      "迭代次数为186，当前损失为1.2166223526000977\n",
      "迭代次数为187，当前损失为1.1817541122436523\n",
      "迭代次数为188，当前损失为1.2200772762298584\n",
      "迭代次数为189，当前损失为1.224759817123413\n",
      "迭代次数为190，当前损失为1.3722885847091675\n",
      "迭代次数为191，当前损失为1.2637078762054443\n",
      "迭代次数为192，当前损失为1.4665099382400513\n",
      "迭代次数为193，当前损失为1.3552299737930298\n",
      "迭代次数为194，当前损失为1.4076677560806274\n",
      "迭代次数为195，当前损失为1.6475799083709717\n",
      "迭代次数为196，当前损失为1.2563577890396118\n",
      "迭代次数为197，当前损失为1.269940733909607\n",
      "迭代次数为198，当前损失为1.2497634887695312\n",
      "迭代次数为199，当前损失为1.2790111303329468\n",
      "迭代次数为200，当前损失为1.2594572305679321\n",
      "迭代次数为201，当前损失为1.1929057836532593\n",
      "迭代次数为202，当前损失为1.4118993282318115\n",
      "迭代次数为203，当前损失为1.3220245838165283\n",
      "迭代次数为204，当前损失为1.3622385263442993\n",
      "迭代次数为205，当前损失为1.651818871498108\n",
      "迭代次数为206，当前损失为1.4484456777572632\n",
      "迭代次数为207，当前损失为1.4479931592941284\n",
      "迭代次数为208，当前损失为1.3043694496154785\n",
      "迭代次数为209，当前损失为1.378804087638855\n",
      "迭代次数为210，当前损失为1.2722015380859375\n",
      "迭代次数为211，当前损失为1.2949076890945435\n",
      "迭代次数为212，当前损失为1.3427488803863525\n",
      "迭代次数为213，当前损失为1.3364275693893433\n",
      "迭代次数为214，当前损失为1.30242919921875\n",
      "迭代次数为215，当前损失为1.3676879405975342\n",
      "迭代次数为216，当前损失为1.2135416269302368\n",
      "迭代次数为217，当前损失为1.234761357307434\n",
      "迭代次数为218，当前损失为1.2952158451080322\n",
      "迭代次数为219，当前损失为1.108564853668213\n",
      "迭代次数为220，当前损失为1.2522742748260498\n",
      "迭代次数为221，当前损失为1.3574753999710083\n",
      "迭代次数为222，当前损失为1.2253727912902832\n",
      "迭代次数为223，当前损失为1.1887348890304565\n",
      "迭代次数为224，当前损失为1.2387033700942993\n",
      "迭代次数为225，当前损失为1.2244125604629517\n",
      "迭代次数为226，当前损失为1.1484252214431763\n",
      "迭代次数为227，当前损失为1.2685744762420654\n",
      "迭代次数为228，当前损失为1.4077695608139038\n",
      "迭代次数为229，当前损失为1.5021476745605469\n",
      "迭代次数为230，当前损失为1.6066726446151733\n",
      "迭代次数为231，当前损失为1.5064772367477417\n",
      "迭代次数为232，当前损失为1.448150873184204\n",
      "迭代次数为233，当前损失为1.5208566188812256\n",
      "迭代次数为234，当前损失为1.409541368484497\n",
      "迭代次数为235，当前损失为1.1256566047668457\n",
      "训练一轮所用时间为： 41.03314601182937\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num =0 \n",
    "for data in train_iter:\n",
    "    x, y = data\n",
    "    y = torch.nn.functional.one_hot(y, 10)\n",
    "    loss = train(network, x, y)\n",
    "    num += 1\n",
    "    print(\"迭代次数为%s，当前损失为%s\" % (num, loss))\n",
    "print(\"训练一轮所用时间为：\", (time.time()-start) / 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的准确率： 0.713100016117096\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集上的准确率：\", (predict(network, test_iter)).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
